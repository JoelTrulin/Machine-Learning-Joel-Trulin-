{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8_6wB4Z82kB"
   },
   "outputs": [],
   "source": [
    "!pip install pandas==2.1.0\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "#!pip install scikit-learn\n",
    "!pip install torch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPJTl74aiiqc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Type, Tuple\n",
    "\n",
    "import datasets\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gatEeRNg_hl"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqnOsG_jEA5A"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'CRPC'\n",
    "\n",
    "def data_processor(df):\n",
    "    df['info'] = df['Title'].astype(str) + df['Abstract'].astype(str)\n",
    "    #           train selection                              irrelevant==20                          relevant==10    dropping group & resetting index\n",
    "    train = df.groupby('Include/Exclude').apply(lambda group: group.head(30) if group.name == 0 else group.head(10)).reset_index(drop=True)\n",
    "\n",
    "    X_train = train['info'].values.reshape(-1)\n",
    "    y_train = train['Include/Exclude'].values.reshape(-1)\n",
    "\n",
    "    #           test selection                                  ratio (irrelevant=2 : relevant=1)    dropping group & resetting index\n",
    "    test = df#.groupby('Include/Exclude').apply(lambda group: group.head(50) if group.name == 0 else group.head(50)).reset_index(drop=True)\n",
    "    X_test = test['info'].values.reshape(-1)\n",
    "    y_test = test['Include/Exclude'].values.reshape(-1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "if dataset_name == 'NSCLC':\n",
    "    df1 = pd.read_excel('/content/drive/MyDrive/Bio-med Roberta/dataset/01. NSCLC - Sent to CapeStart.xlsx',\n",
    "                        usecols=['Title', 'Abstract', 'Include/Exclude'], header=0).replace({'Exclude': 0, 'Include':1}).dropna()\n",
    "    df2 = pd.read_excel('/content/drive/MyDrive/Bio-med Roberta/dataset/09. NSCLC - Sent to CapeStart.xlsx',\n",
    "                        usecols=['Title', 'Abstract', 'Decision'], header=0).replace({'Exclude': 0, 'Include':1}).dropna().rename(columns={'Decision': 'Include/Exclude'})\n",
    "    df3 = pd.read_excel('/content/drive/MyDrive/Bio-med Roberta/dataset/12. NSCLC - Sent to CapeStart.xlsx',\n",
    "                        usecols=['Title', 'Abstract', 'First pass final decision'], header=0).replace(\n",
    "                        {'E1 - Review/editorial': 0, 'E3 - Study design': 0, 'E4 - Intervention': 0, 'E5 - Disease (non-NSCLC)': 0,\n",
    "                        'E6 - Population (non-RET+ NSCLC)': 0,'E7 - Animal/in vitro': 0,'I1 - Include clinical': 1,'I2 - Include EE': 1,\n",
    "                        'I3 - Include HSUV': 1,'I4 - Include cost': 1}).dropna().rename(columns={'First pass final decision': 'Include/Exclude'})\n",
    "    df4 = pd.read_excel('/content/drive/MyDrive/Bio-med Roberta/dataset/A. NSCLC - Sent to CapeStart.xlsx', header=0,\n",
    "                        usecols=['Title', 'Abstract', 'Accept or Reject Code']).replace({'Reject': 0, 'Accept':1}).dropna().rename(\n",
    "                        columns={'Accept or Reject Code': 'Include/Exclude'})\n",
    "    df_  = pd.concat([df1, df2, df3, df4], axis=0)\n",
    "\n",
    "    X_train, y_train, X_test, y_test = data_processor(df_)\n",
    "\n",
    "elif dataset_name == 'COVID':\n",
    "    df_ = pd.read_excel('/content/drive/MyDrive/Bio-med Roberta/dataset/03. COVID - Sent to CapeStart.xlsx',\n",
    "                        usecols=['Title', 'Abstract', 'Include/Exclude'], header=0).replace({'Exclude': 0, 'Include':1}).dropna()\n",
    "    X_train, y_train, X_test, y_test = data_processor(df_)\n",
    "\n",
    "elif dataset_name == 'CRPC':\n",
    "    df_ = pd.read_excel('/content/drive/MyDrive/Bio-med Roberta/dataset/B. CRPC - Sent to CapeStart.xlsx',\n",
    "                          usecols=['Title', 'Abstract', 'Decision'], header=0).replace({'Exclude': 0, 'Include':1}).dropna().rename(columns={'Decision': 'Include/Exclude'})\n",
    "    X_train, y_train, X_test, y_test = data_processor(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-nP4GnBvQtB"
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_12StJCjv-l"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve a single data point and its corresponding label\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        # You may need to perform data transformations here (e.g., convert to tensors)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 5  # You can adjust the batch size as needed\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8PeBHUqyZYD"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdrMAnCByZ4e"
   },
   "outputs": [],
   "source": [
    "device_str = \"cuda\"\n",
    "\n",
    "def train(dl: DataLoader, epochs: int = 3, val_dataset: Dataset = None, recall_weight: int = 8) -> Tuple[collections.OrderedDict, Type[transformers.PreTrainedTokenizer]]:\n",
    "    '''\n",
    "    Trains and returns a model over the data in dl. If a validation Dataset is provided,\n",
    "    the model will be evaluated on this set per epoch, and the model w/the best performance\n",
    "    will be returned; note that 'best' here depends on the `recall_weight', which dictates\n",
    "    how much recall (to class `1', assumed to be includes) is weighted relative to precision.\n",
    "    '''\n",
    "\n",
    "    ''' Model and optimizer '''\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"allenai/biomed_roberta_base\")\n",
    "    model     = RobertaForSequenceClassification.from_pretrained(\"allenai/biomed_roberta_base\",\n",
    "                                                                 num_labels=2).to(device=device_str)\n",
    "\n",
    "    #optimizer = AdamW(model.parameters())\n",
    "    optimizer = optim.Lamb(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    best_val = -np.inf\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"on epoch {epoch}.\")\n",
    "        model.train()\n",
    "        running_losses = []\n",
    "\n",
    "        for batch_num, (X, y) in enumerate(dl):\n",
    "            # print(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_X_tensor = tokenizer.batch_encode_plus(X, max_length=128,\n",
    "                                                        add_special_tokens=True,\n",
    "                                                        pad_to_max_length=True)\n",
    "            batch_y_tensor = torch.tensor(y)\n",
    "            model_outputs = model(torch.tensor(batch_X_tensor['input_ids']).to(device=device_str),\n",
    "                              attention_mask=torch.tensor(batch_X_tensor['attention_mask']).to(device=device_str),\n",
    "                              labels=batch_y_tensor.to(device=device_str))\n",
    "\n",
    "            model_outputs['loss'].backward()\n",
    "\n",
    "            running_losses.append(model_outputs['loss'].detach().float())\n",
    "            if batch_num % 10 == 0:\n",
    "                avg_loss = sum(running_losses[-10:])/len(running_losses[-10:])\n",
    "                print(f\"avg loss for last 10 batches: {avg_loss}\")\n",
    "            optimizer.step()\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            # note that we use the same batchsize for val as for train\n",
    "            # val_dl = DataLoader(val_dataset, batch_size=dl.batch_size)\n",
    "            preds, labels = make_preds(val_dataset, model, tokenizer, device=device_str)\n",
    "            results = classification_eval(preds, labels, threshold=0.5)\n",
    "            # composite score; ad-hoc score,\n",
    "            score = recall_weight*results['recall'][1] + results['precision'][1]\n",
    "            results[\"Ad_hoc_score\"] = score\n",
    "\n",
    "            if score > best_val:\n",
    "                print(\"found new best parameter set; saving.\")\n",
    "                print(\"Classification Report for new best parameter\")\n",
    "                y_preds = np.array(preds)\n",
    "                y_preds = np.where(y_preds > 0.5, 1, 0)\n",
    "                print(metrics.classification_report(labels, y_preds))\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                best_val = score\n",
    "        else:\n",
    "           best_model_state = model.state_dict()\n",
    "\n",
    "    return best_model_state, tokenizer, results\n",
    "\n",
    "def make_preds(val_data: DataLoader, model: Type[torch.nn.Module], tokenizer: Type[transformers.PreTrainedTokenizer], device: str=\"cuda\") -> Tuple:\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (X, y) in val_data:\n",
    "\n",
    "            batch_X_tensor = tokenizer.batch_encode_plus(X, max_length=512,\n",
    "                                                        add_special_tokens=True,\n",
    "                                                        pad_to_max_length=True)\n",
    "            model_outputs = model(torch.tensor(batch_X_tensor['input_ids']).to(device=device),\n",
    "                              attention_mask=torch.tensor(batch_X_tensor['attention_mask']).to(device=device))\n",
    "\n",
    "            probs = torch.softmax(model_outputs['logits'].cuda(), 1)[:,1]\n",
    "            preds.extend(probs.tolist())\n",
    "            labels.extend(y.tolist())\n",
    "\n",
    "    return (preds, labels)\n",
    "\n",
    "# for classification report:-\n",
    "def classification_eval(preds: list, labels: list, threshold: float = 0.5,final = False) -> dict:\n",
    "    y_preds = np.array(preds)\n",
    "    y_preds_binary = np.where(y_preds > threshold, 1, 0)\n",
    "    (p, r, f, s) = metrics.precision_recall_fscore_support(labels, y_preds_binary)\n",
    "    if final == True:\n",
    "        print(metrics.classification_report(labels, y_preds_binary))\n",
    "    return {\"precision\":p, \"recall\":r, \"f\":f}\n",
    "\n",
    "def get_weighted_sampler(dataset: Dataset) -> WeightedRandomSampler:\n",
    "    # total number of positive instances\n",
    "    n = dataset.labels.shape[0]\n",
    "    n_pos = dataset.labels[dataset.labels>0].shape[0]\n",
    "    n_neg = n - n_pos\n",
    "\n",
    "    # split half the mass over the pos examples\n",
    "    pos_weight = 0.5 / n_pos\n",
    "    neg_weight = 0.5 / n_neg\n",
    "\n",
    "    sample_weights = neg_weight * torch.ones(n, dtype=torch.float)\n",
    "    pos_indices = np.argwhere(dataset.labels).squeeze()\n",
    "    sample_weights[pos_indices] = pos_weight\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=n,\n",
    "        replacement=True)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "# for train and save the training model to EFS:-\n",
    "def train_and_save(sr_dataset: Dataset, batch_size: int = 12,\n",
    "                    epochs: int = 10, val_dataset: Dataset = None) -> Tuple:\n",
    "    '''\n",
    "    Trains a classification model on the given review dataset and dumps\n",
    "    to disk. If a val_dataset is provided, performance is evaluated on\n",
    "    this each epoch, and the best model is saved.\n",
    "    '''\n",
    "\n",
    "    # this is a sampler that assigns larger sampling weights to (rare) positive\n",
    "    # examples for batch construction, to account for data imbalance.\n",
    "\n",
    "    # weighted_sampler = get_weighted_sampler(sr_dataset)\n",
    "    # dl = DataLoader(sr_dataset, batch_size=batch_size, sampler=weighted_sampler)\n",
    "    model_state, tokenizer, results = train(sr_dataset, epochs=epochs, val_dataset=val_dataset)\n",
    "    recall = results['recall']\n",
    "\n",
    "    try:\n",
    "        out_path = f'/content/drive/MyDrive/Bio-med Roberta/saved model/{dataset_name}'+'.pt'\n",
    "        print(f\"dumping model weights to {out_path}...\")\n",
    "        torch.save(model_state, out_path)\n",
    "        print(\"Done.\")\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "      print('Failed')\n",
    "    #     ndata = {\"Status code\": \"400\",\n",
    "    #              \"Status\": \"Training failed\",\n",
    "    #              \"Description\": \"Data has failed to train and dump model...\"\n",
    "    #             }\n",
    "    #     return Response(ndata)\n",
    "    return (recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVE3mW5pOJMv"
   },
   "outputs": [],
   "source": [
    "train_and_save(train_dataloader, 5, 20, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQUvD_OI3DRA"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1695013151181,
     "user": {
      "displayName": "Joel Trulin J",
      "userId": "11165460193007417230"
     },
     "user_tz": -330
    },
    "id": "G4vRBLndDq5z",
    "outputId": "d3ae3492-50e7-4a6e-901a-3db8efcb3c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1695013261834,
     "user": {
      "displayName": "Joel Trulin J",
      "userId": "11165460193007417230"
     },
     "user_tz": -330
    },
    "id": "aubqor1i1bSR",
    "outputId": "fb72fe54-82b3-4dc8-9bdd-3e1cceda0726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn.h: /usr/include/cudnn.h\n"
     ]
    }
   ],
   "source": [
    "!whereis cudnn.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrwATnXaqGki"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPzd7fAhRBw1YB7c7T5ZSs4",
   "mount_file_id": "1aDtUbiwS6BKyaZc_rKVQkdGRZIaGlV-b",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
