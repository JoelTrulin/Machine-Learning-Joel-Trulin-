{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T05:34:05.813470625Z",
     "start_time": "2023-11-13T05:34:02.796330564Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 05:18:17.946746: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 05:18:17.946797: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 05:18:17.946820: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENCV_LOG_LEVEL\"] = \"SILENT\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import cv2\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from keras import layers\n",
    "from Segmentation_classification.utils.pconv_layer import PConv2D\n",
    "# from keras.src import backend\n",
    "# backend.update\n",
    "from keras.applications.efficientnet_v2 import EfficientNetV2B0\n",
    "from livelossplot import PlotLossesKeras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dvclive import Live\n",
    "from dvclive.keras import DVCLiveCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:29:37.067557904Z",
     "start_time": "2023-11-12T17:27:50.293581566Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.setNumThreads(cv2.getNumThreads())\n",
    "\n",
    "def extract(mode='train', dim=256):\n",
    "\n",
    "    def gen_mask(polygon_coords):\n",
    "        width, height = dim, dim\n",
    "        # image = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        # Create a polygon mask\n",
    "        polygon_mask = np.zeros((height, width), dtype=np.int32)\n",
    "        for val, coord in enumerate(polygon_coords, 1):\n",
    "            cv2.fillPoly(polygon_mask, [coord], val)\n",
    "        return polygon_mask\n",
    "\n",
    "    def preproc_polycoord(coord):  # convert and reshape float coordinates into int\n",
    "        poly_coord_ = np.array(coord).reshape((-1, 2))  # reshape to (x, y)\n",
    "        # convert coordinates range in (0-1) to image shape ie 256\n",
    "        poly_coord_ *= dim\n",
    "        poly_coord_ = np.round(poly_coord_).astype('int32')\n",
    "        return poly_coord_\n",
    "\n",
    "    X, Y1, Y2 = [], [], []\n",
    "    count = 0\n",
    "    for file in os.scandir(f'dataset/full_teeth/{mode}/labels'):\n",
    "        with open(file.path, 'r') as dt:\n",
    "            text = dt.read()\n",
    "\n",
    "        if len(np_txt := text.split('\\n')) == 2:\n",
    "            cls_txt = [int(np.fromstring(i, sep=' ')[0]) for i in np_txt]\n",
    "            np_txt = [np.fromstring(i, sep=' ')[1:] for i in np_txt]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        poly_coord = [preproc_polycoord(i) for i in np_txt]\n",
    "        mask_img = gen_mask(poly_coord)\n",
    "        # plt.imshow(mask_img , cmap='gray')\n",
    "        # plt.show()\n",
    "\n",
    "        img = cv2.imread(file.path.replace(\n",
    "            'labels', 'images').split('.')[0]+'.jpeg')\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (dim, dim))\n",
    "            X.append(img)\n",
    "            Y1.append(mask_img)\n",
    "            Y2.append(cls_txt)\n",
    "            count += 1\n",
    "        elif count > 100000:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    Y2 = np.array(Y2)\n",
    "    return np.array(X).astype(np.int32), np.array(Y1), Y2[:, 0], Y2[:, 1]\n",
    "\n",
    "dim= 256\n",
    "X_train, Y_train1, Y_train2, Y_train3 = extract(dim=dim)\n",
    "# X_test, Y_test1, Y_test2, Y_test3 = extract(mode='test', dim=dim)\n",
    "X_val, Y_val1, Y_val2, Y_val3 = extract(mode='validate', dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Segmentation & Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:29:41.790755848Z",
     "start_time": "2023-11-12T17:29:37.067347581Z"
    }
   },
   "outputs": [],
   "source": [
    "class SegmentImg(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_img_, y_, segment_region = inputs\n",
    "\n",
    "        x_img_ = tf.cast(x_img_, dtype=tf.int32)\n",
    "        y_ = tf.cast(y_, dtype=tf.int32)\n",
    "        segment_region = tf.cast(segment_region, dtype=tf.int32)\n",
    "\n",
    "        mask = tf.where(tf.logical_or(tf.equal(y_, 0),\n",
    "                        tf.equal(y_, segment_region)))\n",
    "        mask_shape = tf.shape(mask)\n",
    "        mask_shape = tf.tensor_scatter_nd_update(mask_shape, [[1]], updates =[3])\n",
    "        update = tf.fill(mask_shape, 0)\n",
    "        # mask_update = tf.tile(tf.expand_dims(update, axis=-1), [1, 3])\n",
    "        return tf.cast(tf.tensor_scatter_nd_update(x_img_, mask, updates=update), dtype=tf.float32)\n",
    "\n",
    "class MyByte(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = EfficientNetV2B0(include_top=False, input_shape=(\n",
    "            dim, dim, 3), include_preprocessing=False)\n",
    "        # Block 1\n",
    "        self.upsamp_1 = layers.UpSampling2D((2, 2))\n",
    "        self.conv_1 = layers.Conv2DTranspose(\n",
    "            1280, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_2 = layers.Conv2DTranspose(\n",
    "            1280, (2, 2), padding='same', activation='relu')\n",
    "        # Block 2\n",
    "        self.upsamp_2 = layers.UpSampling2D((2, 2))\n",
    "        self.conv_3 = layers.Conv2DTranspose(\n",
    "            640, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_4 = layers.Conv2DTranspose(\n",
    "            640, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_5 = layers.Conv2DTranspose(\n",
    "            640, (2, 2), padding='same', activation='relu')\n",
    "        # Block 3\n",
    "        self.upsamp_3 = layers.UpSampling2D((2, 2))\n",
    "        self.conv_6 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_7 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_8 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_9 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        # Block 4\n",
    "        self.upsamp_4 = layers.UpSampling2D((2, 2))\n",
    "        self.conv_10 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_11 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_12 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_13 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_14 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        # Block 5\n",
    "        self.upsamp_5 = layers.UpSampling2D((2, 2))\n",
    "        self.conv_15 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_16 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_17 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_18 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_19 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_20 = layers.Conv2DTranspose(\n",
    "            512, (2, 2), padding='same', activation='relu')\n",
    "        self.conv_21 = layers.Conv2DTranspose(\n",
    "            3, (1, 1), activation='softmax')\n",
    "        \n",
    "        # Segmentating input image based on predicted masked image\n",
    "        self.segment_img = SegmentImg()\n",
    "        # argmax of segmentation and masking\n",
    "        self.lambdaa = layers.Lambda(lambda u: tf.argmax(u, axis=3))\n",
    "        self.masking_layer = layers.Masking(mask_value=0, input_shape=(3,))\n",
    "        # Classification\n",
    "        self.multiheadattention1 = layers.MultiHeadAttention(num_heads=2, key_dim=4)\n",
    "        self.multiheadattention2 = layers.MultiHeadAttention(num_heads=2, key_dim=4)\n",
    "        self.add = layers.Add() \n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "\n",
    "        # pooling\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(256, activation='relu')\n",
    "        self.dense2 = layers.Dense(256, activation='relu')\n",
    "        self.denseout1 = layers.Dense(4, activation='softmax')\n",
    "        self.denseout2 = layers.Dense(4, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "\n",
    "        if training:\n",
    "            x_img, y = inputs\n",
    "        else:\n",
    "            x_img, y_mask = inputs\n",
    "\n",
    "        \"\"\"x = self.base_model(x_img)\n",
    "        x = self.upsamp_1(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.upsamp_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.conv_5(x)\n",
    "        x = self.upsamp_3(x)\n",
    "        x = self.conv_6(x)\n",
    "        x = self.conv_7(x)\n",
    "        x = self.conv_8(x)\n",
    "        x = self.conv_9(x)\n",
    "        x = self.upsamp_4(x)\n",
    "        x = self.conv_10(x)\n",
    "        x = self.conv_11(x)\n",
    "        x = self.conv_12(x)\n",
    "        x = self.conv_13(x)\n",
    "        x = self.conv_14(x)\n",
    "        x = self.upsamp_5(x)\n",
    "        x = self.conv_15(x)\n",
    "        x = self.conv_16(x)\n",
    "        x = self.conv_17(x)\n",
    "        x = self.conv_18(x)\n",
    "        x = self.conv_19(x)\n",
    "        x = self.conv_20(x)\n",
    "        segmentation_output = self.conv_21(x)\"\"\"\n",
    "\n",
    "\n",
    "        if training:\n",
    "            masked_img1 = self.segment_img([x_img, y, 2])\n",
    "            masked_img2 = self.segment_img([x_img, y, 1])\n",
    "\n",
    "            clss1 = self.masking_layer(masked_img1)\n",
    "            for _ in range(5):\n",
    "                attn_out1 = self.multiheadattention1(clss1, clss1, use_causal_mask=True)\n",
    "                clss1 = self.add([clss1, attn_out1])\n",
    "                clss1 = self.layernorm(clss1)\n",
    "            clss1 = self.flatten(clss1)\n",
    "            # clss1 = self.dropout(clss1)\n",
    "            clss1 = self.dense1(clss1)\n",
    "            output1 = self.denseout1(clss1)\n",
    "\n",
    "            clss2 = self.masking_layer(masked_img2)\n",
    "            for _ in range(5):\n",
    "                attn_out2 = self.multiheadattention2(clss2, clss2, use_causal_mask=True)\n",
    "                clss2 = self.add([clss2, attn_out2])\n",
    "                clss2 = self.layernorm(clss2)\n",
    "            clss2 = self.flatten(clss2)\n",
    "            # clss2 = self.dropout(clss2)\n",
    "            clss2 = self.dense2(clss2)\n",
    "            output2 = self.denseout2(clss2)\n",
    "\n",
    "        else:\n",
    "            # y_mask = self.lambdaa(segmentation_output)\n",
    "            masked_img1 = self.segment_img([x_img, y_mask, 2])\n",
    "            masked_img2 = self.segment_img([x_img, y_mask, 1])\n",
    "\n",
    "            clss1 = self.masking_layer(masked_img1)\n",
    "            for _ in range(1):\n",
    "                attn_out1 = self.multiheadattention1(clss1, clss1, use_causal_mask=True)\n",
    "                clss1 = self.add([clss1, attn_out1])\n",
    "                clss1 = self.layernorm(clss1)\n",
    "            clss1 = self.flatten(clss1)\n",
    "            # clss1 = self.dropout(clss1)\n",
    "            clss1 = self.dense1(clss1)\n",
    "            output1 = self.denseout1(clss1)\n",
    "\n",
    "            clss2 = self.masking_layer(masked_img2)\n",
    "            for _ in range(1):\n",
    "                attn_out2 = self.multiheadattention2(clss2, clss2, use_causal_mask=True)\n",
    "                clss2 = self.add([clss2, attn_out2])\n",
    "                clss2 = self.layernorm(clss2)\n",
    "            clss2 = self.flatten(clss2)\n",
    "            # clss2 = self.dropout(clss2)\n",
    "            clss2 = self.dense2(clss2)\n",
    "            output2 = self.denseout2(clss2)\n",
    "\n",
    "        # return [segmentation_output, output1, output2]\n",
    "        return [output1, output2]\n",
    "\n",
    "\n",
    "# Create an instance of your custom model\n",
    "model = MyByte()\n",
    "# model.output_names=['seg', 'clss1', 'clss2']\n",
    "model.output_names=['clss1', 'clss2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 256, 256)]           0         []                            \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256)]           0         ['input_6[0][0]']             \n",
      " Cast_1 (TensorFlowOpLayer)                                                                       \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256)]           0         ['input_6[0][0]']             \n",
      " Cast_1 (TensorFlowOpLayer)                                                                       \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256)]           0         ['tf_op_layer_segment_img_4/Ca\n",
      " Equal (TensorFlowOpLayer)                                          st_1[0][0]']                  \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256)]           0         ['tf_op_layer_segment_img_4/Ca\n",
      " Equal_1 (TensorFlowOpLayer                                         st_1[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256)]           0         ['tf_op_layer_segment_img_5/Ca\n",
      " Equal (TensorFlowOpLayer)                                          st_1[0][0]']                  \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256)]           0         ['tf_op_layer_segment_img_5/Ca\n",
      " Equal_1 (TensorFlowOpLayer                                         st_1[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256)]           0         ['tf_op_layer_segment_img_4/Eq\n",
      " LogicalOr (TensorFlowOpLay                                         ual[0][0]',                   \n",
      " er)                                                                 'tf_op_layer_segment_img_4/Eq\n",
      "                                                                    ual_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256)]           0         ['tf_op_layer_segment_img_5/Eq\n",
      " LogicalOr (TensorFlowOpLay                                         ual[0][0]',                   \n",
      " er)                                                                 'tf_op_layer_segment_img_5/Eq\n",
      "                                                                    ual_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 3)]                  0         ['tf_op_layer_segment_img_4/Lo\n",
      " Where (TensorFlowOpLayer)                                          gicalOr[0][0]']               \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 3)]                  0         ['tf_op_layer_segment_img_5/Lo\n",
      " Where (TensorFlowOpLayer)                                          gicalOr[0][0]']               \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(2,)]                       0         ['tf_op_layer_segment_img_4/Wh\n",
      " Shape (TensorFlowOpLayer)                                          ere[0][0]']                   \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(2,)]                       0         ['tf_op_layer_segment_img_5/Wh\n",
      " Shape (TensorFlowOpLayer)                                          ere[0][0]']                   \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(2,)]                       0         ['tf_op_layer_segment_img_4/Sh\n",
      " TensorScatterUpdate (Tenso                                         ape[0][0]']                   \n",
      " rFlowOpLayer)                                                                                    \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(2,)]                       0         ['tf_op_layer_segment_img_5/Sh\n",
      " TensorScatterUpdate (Tenso                                         ape[0][0]']                   \n",
      " rFlowOpLayer)                                                                                    \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256, 3)]        0         ['input_5[0][0]']             \n",
      " Cast (TensorFlowOpLayer)                                                                         \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, None)]               0         ['tf_op_layer_segment_img_4/Te\n",
      " Fill (TensorFlowOpLayer)                                           nsorScatterUpdate[0][0]']     \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256, 3)]        0         ['input_5[0][0]']             \n",
      " Cast (TensorFlowOpLayer)                                                                         \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, None)]               0         ['tf_op_layer_segment_img_5/Te\n",
      " Fill (TensorFlowOpLayer)                                           nsorScatterUpdate[0][0]']     \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256, 3)]        0         ['tf_op_layer_segment_img_4/Ca\n",
      " TensorScatterUpdate_1 (Ten                                         st[0][0]',                    \n",
      " sorFlowOpLayer)                                                     'tf_op_layer_segment_img_4/Wh\n",
      "                                                                    ere[0][0]',                   \n",
      "                                                                     'tf_op_layer_segment_img_4/Fi\n",
      "                                                                    ll[0][0]']                    \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256, 3)]        0         ['tf_op_layer_segment_img_5/Ca\n",
      " TensorScatterUpdate_1 (Ten                                         st[0][0]',                    \n",
      " sorFlowOpLayer)                                                     'tf_op_layer_segment_img_5/Wh\n",
      "                                                                    ere[0][0]',                   \n",
      "                                                                     'tf_op_layer_segment_img_5/Fi\n",
      "                                                                    ll[0][0]']                    \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_4/  [(None, 256, 256, 3)]        0         ['tf_op_layer_segment_img_4/Te\n",
      " Cast_2 (TensorFlowOpLayer)                                         nsorScatterUpdate_1[0][0]']   \n",
      "                                                                                                  \n",
      " tf_op_layer_segment_img_5/  [(None, 256, 256, 3)]        0         ['tf_op_layer_segment_img_5/Te\n",
      " Cast_2 (TensorFlowOpLayer)                                         nsorScatterUpdate_1[0][0]']   \n",
      "                                                                                                  \n",
      " masking_4 (Masking)         (None, 256, 256, 3)          0         ['tf_op_layer_segment_img_4/Ca\n",
      "                                                                    st_2[0][0]']                  \n",
      "                                                                                                  \n",
      " masking_5 (Masking)         (None, 256, 256, 3)          0         ['tf_op_layer_segment_img_5/Ca\n",
      "                                                                    st_2[0][0]']                  \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (M  (None, 256, 256, 3)          123       ['masking_4[0][0]',           \n",
      " ultiHeadAttention)                                                  'masking_4[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_25 (M  (None, 256, 256, 3)          123       ['masking_5[0][0]',           \n",
      " ultiHeadAttention)                                                  'masking_5[0][0]']           \n",
      "                                                                                                  \n",
      " add_20 (Add)                (None, 256, 256, 3)          0         ['masking_4[0][0]',           \n",
      "                                                                     'multi_head_attention_20[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_25 (Add)                (None, 256, 256, 3)          0         ['masking_5[0][0]',           \n",
      "                                                                     'multi_head_attention_25[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_20 (La  (None, 256, 256, 3)          6         ['add_20[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_25 (La  (None, 256, 256, 3)          6         ['add_25[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_21 (M  (None, 256, 256, 3)          123       ['layer_normalization_20[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_26 (M  (None, 256, 256, 3)          123       ['layer_normalization_25[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_21 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_21[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_26 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_25[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_26[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_21 (La  (None, 256, 256, 3)          6         ['add_21[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_26 (La  (None, 256, 256, 3)          6         ['add_26[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (M  (None, 256, 256, 3)          123       ['layer_normalization_21[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_27 (M  (None, 256, 256, 3)          123       ['layer_normalization_26[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_22 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_21[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_27 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_26[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_27[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_22 (La  (None, 256, 256, 3)          6         ['add_22[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_27 (La  (None, 256, 256, 3)          6         ['add_27[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_23 (M  (None, 256, 256, 3)          123       ['layer_normalization_22[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_28 (M  (None, 256, 256, 3)          123       ['layer_normalization_27[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_23 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_22[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_23[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_28 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_27[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_28[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_23 (La  (None, 256, 256, 3)          6         ['add_23[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_28 (La  (None, 256, 256, 3)          6         ['add_28[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_24 (M  (None, 256, 256, 3)          123       ['layer_normalization_23[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_29 (M  (None, 256, 256, 3)          123       ['layer_normalization_28[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_24 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_23[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_24[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_29 (Add)                (None, 256, 256, 3)          0         ['layer_normalization_28[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'multi_head_attention_29[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_24 (La  (None, 256, 256, 3)          6         ['add_24[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_29 (La  (None, 256, 256, 3)          6         ['add_29[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)         (None, 196608)               0         ['layer_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)         (None, 196608)               0         ['layer_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 256)                  5033190   ['flatten_4[0][0]']           \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 256)                  5033190   ['flatten_5[0][0]']           \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " clss1 (Dense)               (None, 4)                    1028      ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " clss2 (Dense)               (None, 4)                    1028      ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 100667154 (384.01 MB)\n",
      "Trainable params: 100667154 (384.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class SegmentImg(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_img_, y_, segment_region = inputs\n",
    "\n",
    "        x_img_ = tf.cast(x_img_, dtype=tf.int32)\n",
    "        y_ = tf.cast(y_, dtype=tf.int32)\n",
    "        segment_region = tf.cast(segment_region, dtype=tf.int32)\n",
    "\n",
    "        mask = tf.where(tf.logical_or(tf.equal(y_, 0),\n",
    "                        tf.equal(y_, segment_region)))\n",
    "        mask_shape = tf.shape(mask)\n",
    "        mask_shape = tf.tensor_scatter_nd_update(mask_shape, [[1]], updates =[3])\n",
    "        update = tf.fill(mask_shape, 0)\n",
    "        # mask_update = tf.tile(tf.expand_dims(update, axis=-1), [1, 3])\n",
    "        return tf.cast(tf.tensor_scatter_nd_update(x_img_, mask, updates=update), dtype=tf.float32)\n",
    "\n",
    "input1 = layers.Input((dim, dim, 3))\n",
    "input2 = layers.Input((dim, dim))\n",
    "\n",
    "masked_img1 = SegmentImg()([input1, input2, 2])\n",
    "masked_img2 = SegmentImg()([input1, input2, 1])\n",
    "clss1 = layers.Masking(mask_value=0, input_shape=(3,))(masked_img1)\n",
    "for _ in range(5):\n",
    "    attn_out1 = layers.MultiHeadAttention(num_heads=2, key_dim=4)(clss1, clss1, use_causal_mask=True)\n",
    "    clss1 = layers.Add()([clss1, attn_out1])\n",
    "    clss1 = layers.LayerNormalization()(clss1)\n",
    "clss1 = layers.Flatten()(clss1)\n",
    "# clss1 = self.dropout(clss1)\n",
    "clss1 = layers.Dense(256, activation='relu')(clss1)\n",
    "output1 = layers.Dense(4, activation='softmax', name='clss1')(clss1)\n",
    "\n",
    "clss2 = layers.Masking(mask_value=0, input_shape=(3,))(masked_img2)\n",
    "for _ in range(5):\n",
    "    attn_out2 = layers.MultiHeadAttention(num_heads=2, key_dim=4)(clss2, clss2, use_causal_mask=True)\n",
    "    clss2 = layers.Add()([clss2, attn_out2])\n",
    "    clss2 = layers.LayerNormalization()(clss2)\n",
    "clss2 = layers.Flatten()(clss2)\n",
    "# clss2 = self.dropout(clss2)\n",
    "clss2 = layers.Dense(256, activation='relu')(clss2)\n",
    "output2 = layers.Dense(4, activation='softmax', name='clss2')(clss2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input1, input2], outputs=[output1, output2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 509 samples, validate on 113 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The following untracked files were present in the workspace before saving but will not be included in the experiment commit:\n",
      "\tMultiheadAttention.ipynb, 768Pconv_model.keras.dvc, output.png\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,2,65536,65536] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node multi_head_attention_10/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[loss_1/Identity/_1097]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,2,65536,65536] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node multi_head_attention_10/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m               loss\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclss1\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mclss2\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m               metrics\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclss1\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclss2\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mwith\u001b[39;00m Live() \u001b[39mas\u001b[39;00m live:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m       model\u001b[39m.\u001b[39;49mfit([X_train, Y_train1], {\u001b[39m'\u001b[39;49m\u001b[39mclss1\u001b[39;49m\u001b[39m'\u001b[39;49m: Y_train2, \u001b[39m'\u001b[39;49m\u001b[39mclss2\u001b[39;49m\u001b[39m'\u001b[39;49m: Y_train3}, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39;49m([X_val, Y_val1], [Y_val2, Y_val3]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m                 callbacks\u001b[39m=\u001b[39;49m[PlotLossesKeras(), DVCLiveCallback(live\u001b[39m=\u001b[39;49mlive)])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m       model\u001b[39m.\u001b[39msave(model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.keras\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d792d42797465227d/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/MultiheadAttention.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m       live\u001b[39m.\u001b[39mlog_artifact(model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.keras\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mmodel_name)\n",
      "File \u001b[0;32m/newvolume/mybyte/Joel/venv/lib/python3.10/site-packages/keras/src/engine/training_v1.py:856\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    855\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 856\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    857\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    858\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    859\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    860\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    861\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    862\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    863\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    864\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    865\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    866\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    867\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    868\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    869\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    870\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    871\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    872\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    873\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    874\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    875\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    876\u001b[0m )\n",
      "File \u001b[0;32m/newvolume/mybyte/Joel/venv/lib/python3.10/site-packages/keras/src/engine/training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_steps` should not be specified if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_data` is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[39mreturn\u001b[39;00m fit_loop(\n\u001b[1;32m    735\u001b[0m     model,\n\u001b[1;32m    736\u001b[0m     inputs\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    737\u001b[0m     targets\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    738\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    739\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    740\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    741\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    742\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    743\u001b[0m     val_inputs\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m    744\u001b[0m     val_targets\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m    745\u001b[0m     val_sample_weights\u001b[39m=\u001b[39;49mval_sample_weights,\n\u001b[1;32m    746\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    747\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    748\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    749\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    750\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    751\u001b[0m     steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    752\u001b[0m )\n",
      "File \u001b[0;32m/newvolume/mybyte/Joel/venv/lib/python3.10/site-packages/keras/src/engine/training_arrays_v1.py:421\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m callbacks\u001b[39m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    417\u001b[0m     mode, \u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    418\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m batch_outs \u001b[39m=\u001b[39m f(ins_batch)\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_outs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    423\u001b[0m     batch_outs \u001b[39m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m/newvolume/mybyte/Joel/venv/lib/python3.10/site-packages/keras/src/backend.py:4609\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4599\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4600\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4601\u001b[0m     \u001b[39mor\u001b[39;00m feed_arrays \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4605\u001b[0m     \u001b[39mor\u001b[39;00m session \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\n\u001b[1;32m   4606\u001b[0m ):\n\u001b[1;32m   4607\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4609\u001b[0m fetched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callable_fn(\u001b[39m*\u001b[39;49marray_vals, run_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_metadata)\n\u001b[1;32m   4610\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches) :])\n\u001b[1;32m   4611\u001b[0m output_structure \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4612\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4613\u001b[0m     fetched[: \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)],\n\u001b[1;32m   4614\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   4615\u001b[0m )\n",
      "File \u001b[0;32m/newvolume/mybyte/Joel/venv/lib/python3.10/site-packages/tensorflow/python/client/session.py:1505\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1504\u001b[0m   run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1505\u001b[0m   ret \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_SessionRunCallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_session,\n\u001b[1;32m   1506\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, args,\n\u001b[1;32m   1507\u001b[0m                                          run_metadata_ptr)\n\u001b[1;32m   1508\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1509\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,2,65536,65536] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node multi_head_attention_10/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[loss_1/Identity/_1097]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,2,65536,65536] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node multi_head_attention_10/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "model_name = str(dim)+'Attention_model_without_segment'\n",
    "batch_size = 1\n",
    "\"\"\"model.compile(optimizer=tf.optimizers.RMSprop(learning_rate=0.00001),\n",
    "              loss={'seg': 'sparse_categorical_crossentropy', 'clss1': 'sparse_categorical_crossentropy',\n",
    "                    'clss2': 'sparse_categorical_crossentropy'},\n",
    "              metrics={'seg': 'accuracy', 'clss1': 'accuracy', 'clss2': 'accuracy'})\n",
    "with Live() as live:\n",
    "      model.fit([X_train, Y_train1], {'seg': Y_train1, 'clss1': Y_train2, 'clss2': Y_train3}, batch_size=batch_size, epochs=50, verbose=1,\n",
    "          validation_data=(X_val, [Y_val1, Y_val2, Y_val3]), validation_batch_size=batch_size,\n",
    "          callbacks=[PlotLossesKeras(), DVCLiveCallback(live=live)])\n",
    "\n",
    "      model.save(model_name + '.keras')\n",
    "      live.log_artifact(model_name + '.keras', type=model_name)\"\"\"\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'clss1': 'sparse_categorical_crossentropy',\n",
    "                    'clss2': 'sparse_categorical_crossentropy'},\n",
    "              metrics={'clss1': 'accuracy', 'clss2': 'accuracy'})\n",
    "with Live() as live:\n",
    "      model.fit([X_train, Y_train1], {'clss1': Y_train2, 'clss2': Y_train3}, batch_size=batch_size, epochs=50, verbose=1,\n",
    "                validation_data=([X_val, Y_val1], [Y_val2, Y_val3]),\n",
    "                callbacks=[PlotLossesKeras(), DVCLiveCallback(live=live)])\n",
    "\n",
    "      model.save(model_name + '.keras')\n",
    "      live.log_artifact(model_name + '.keras', type=model_name)\n",
    "# loss, accuracy = model.evaluate(X_test, [Y_test1, Y_test2, Y_test3])\n",
    "# error = 197871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T22:35:11.623993439Z",
     "start_time": "2023-11-12T22:35:08.450588029Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "hist_df = pd.DataFrame(model.history.history)\n",
    "hist_df.to_csv(model_name + '.csv', index=False)\n",
    "# model.save(\"seg_clss_iter1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "256*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('segmentation.tf')\n",
    "img = cv2.imread(\n",
    "    \"fullteeth/test/images/0015w00002NZScQAAX_4dff1fe0-4fc1-11eb-9072-8fb3c6ce79ed-checkinsmileon-1687979590788.jpeg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_res = cv2.resize(img, (256, 256)).reshape((1, 256, 256, 3))\n",
    "# pred = model.predict(img_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_img = np.argmax(pred, 3).reshape((256, 256))\n",
    "plt.imshow(img_res.reshape((256, 256, 3)))\n",
    "plt.show()\n",
    "plt.imshow(pred_img.astype(np.uint))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "resized_img = np.copy(img_res)\n",
    "contours, _ = cv2.findContours(pred_img.astype(\n",
    "    np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "# Convert the contour to a list of polygon coordinates\n",
    "polygon_coords = largest_contour.reshape(-1, 2)\n",
    "# Optionally, you can round the coordinates to integer values\n",
    "polygon_coords = np.round(polygon_coords).astype(int)\n",
    "# Print or use the polygon coordinates\n",
    "print(\"Polygon Coordinates:\")\n",
    "cv2.drawContours(resized_img, [polygon_coords], -1,\n",
    "                 (255, 255, 255), thickness=cv2.FILLED)\n",
    "plt.imshow(resized_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
