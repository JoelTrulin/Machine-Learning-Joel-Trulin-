{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 08:12:44.170512: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 08:12:44.170547: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 08:12:44.170565: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "os.environ[\"OPENCV_LOG_LEVEL\"] = \"SILENT\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for devices in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(devices, True)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import tflite_runtime.interpreter as tflite\n",
    "from dvclive import Live\n",
    "import onnxruntime as ort\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27252 files belonging to 5 classes.\n",
      "Using 20167 files for training.\n",
      "Using 7085 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    }
   ],
   "source": [
    "dim =64\n",
    "image_size = (dim, dim)\n",
    "batch_size = 150\n",
    "\n",
    "train_ds, other = keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/dataset_checkin\",\n",
    "    validation_split=0.26,\n",
    "    subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# train_ds = train_ds.map(lambda x, y: (tf.cast(x, tf.uint8), y))\n",
    "\n",
    "\n",
    "bch = other.cardinality().numpy()\n",
    "bch /= 2\n",
    "test_ds, val_ds = other.take(bch), other.skip(bch)\n",
    "# test_ds = test_ds.map(lambda x, y: (tf.cast(x, tf.uint8), y))\n",
    "\n",
    "y_test = np.concatenate([y.numpy() for _, y in test_ds])\n",
    "label = {0:'left', 1:'lower', 2:'right', 3:'smile', 4:'upper'}\n",
    "y_test = [label[i] for i in y_test]\n",
    "\n",
    "X_test = np.concatenate([x.numpy() for x, _ in test_ds])\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = list()\n",
    "cap = cv2.VideoCapture('Video/one.MOV')\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    Images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "Images_resized = np.array(list(map(lambda x: cv2.resize(x, (768, 768)), Images)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFlite\n",
    "Model Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "Data loaded\n",
      "converting\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp_p8mharb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_p8mharb/assets\n",
      "/my-byte/Joel/faceid_venv/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:947: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value, _ in train_ds.take(10):\n",
    "    yield [input_value]\n",
    "\n",
    "keras_model = keras.models.load_model('Models/64Efficientnetv2B0.keras')\n",
    "print('loaded')\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)  # path to the SavedModel directory\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "print('Data loaded')\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input utput tensors to uint8\n",
    "converter.inference_input_type = tf.uint8\n",
    "# converter.inference_output_type = tf.int16\n",
    "print('converting')\n",
    "tflite_model = converter.convert()\n",
    "# Save the model.\n",
    "with open('Models/64Efficientnetv2B0_quint8.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(inference pid=18008) INFO: Created TensorFlow Lite XNNPACK delegate for CPU. [repeated 23x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def inference(Image):\n",
    "    model = tflite.Interpreter(model_path='Models/64_8L524K_qAct16W8.tflite')\n",
    "\n",
    "    model.allocate_tensors()\n",
    "    input_details = model.get_input_details()\n",
    "\n",
    "    model.set_tensor(input_details[0]['index'], Image)\n",
    "\n",
    "    # Run inference\n",
    "    model.invoke()\n",
    "\n",
    "    # Get the output\n",
    "    output_details = model.get_output_details()\n",
    "    pred = model.get_tensor(output_details[0]['index'])\n",
    "    return pred[0]\n",
    "\n",
    "futures = [inference.remote(i) for i in X_test]\n",
    "# Retrieve results.\n",
    "pred_arr = ray.get(futures)\n",
    "ray.shutdown()\n",
    "y_pred = np.argmax(pred_arr, axis=1)\n",
    "y_pred = [label[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Ray Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflite.Interpreter(model_path='Model/64_8L524K_quint8.tflite', num_threads=10)\n",
    "batch = 30\n",
    "def inference(Image):\n",
    "    model.resize_tensor_input(0, [batch, 768, 768, 3])\n",
    "    model.allocate_tensors()\n",
    "    input_details = model.get_input_details()\n",
    "\n",
    "    model.set_tensor(input_details[0]['index'], Image)\n",
    "\n",
    "    # Run inference\n",
    "    model.invoke()\n",
    "\n",
    "    # Get the output\n",
    "    output_details = model.get_output_details()\n",
    "    pred = model.get_tensor(output_details[0]['index'])\n",
    "    return pred\n",
    "\n",
    "prediction = inference(X_test[:batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dvclive:'64_8L524k/dvc.yaml' is in outputs of stage '64_8L524k.dvc'.\n",
      "Remove it from outputs to make DVCLive work as expected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Folder = '64_8L524k'\n",
    "model_name = '64_8L524K_qAct16W8'\n",
    "model_path = 'Models/'\n",
    "with Live(dir=Folder+'/dvclive/', dvcyaml=Folder + '/dvc.yaml',\n",
    "          exp_name='64_8L524k', resume=True) as live:\n",
    "    live.log_artifact('Models/64_8L524K_qAct16W8.tflite', type='model', name=model_name)\n",
    "    live.log_sklearn_plot(\"confusion_matrix\", y_test, y_pred, name='64_8L524K_qAct16W8_tflite', title='64_8L524K_qAct16W8_tflite')\n",
    "    # live.end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.93      0.94      0.93       505\n",
      "       lower       0.91      0.92      0.91       859\n",
      "       right       0.96      0.93      0.95       506\n",
      "       smile       0.89      0.92      0.91       772\n",
      "       upper       0.94      0.92      0.93       958\n",
      "\n",
      "    accuracy                           0.92      3600\n",
      "   macro avg       0.93      0.92      0.92      3600\n",
      "weighted avg       0.92      0.92      0.92      3600\n",
      "\n",
      "[[474   7   4  16   4]\n",
      " [  5 786   8  23  37]\n",
      " [  5   9 471  16   5]\n",
      " [ 22  21   4 711  14]\n",
      " [  6  44   2  29 877]]\n",
      "[[92.578125    0.80738178  0.81799591  2.01257862  0.42689434]\n",
      " [ 0.9765625  90.65743945  1.63599182  2.89308176  3.94877268]\n",
      " [ 0.9765625   1.03806228 96.3190184   2.01257862  0.53361793]\n",
      " [ 4.296875    2.42214533  0.81799591 89.43396226  1.4941302 ]\n",
      " [ 1.171875    5.07497116  0.40899796  3.64779874 93.59658485]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm, cm/np.sum(cm, axis=0)*100, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2024-02-08 08:34:34,279 - INFO - Using tensorflow=2.13.1, onnx=1.15.0, tf2onnx=1.16.1/15c810\n",
      "2024-02-08 08:34:34,279 - INFO - Using opset <onnx, 13>\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2024-02-08 08:34:35,706 - INFO - Optimizing ONNX model\n",
      "2024-02-08 08:34:55,741 - INFO - After optimization: Cast -9 (9->0), Const -1709 (2518->809), DequantizeLinear -64 (664->600), Identity -2 (2->0), QuantizeLinear -56 (427->371), Transpose -483 (486->3)\n",
      "2024-02-08 08:34:55,894 - INFO - \n",
      "2024-02-08 08:34:55,894 - INFO - Successfully converted TensorFlow model Models/Fitment_Prediction/best_saved_model/best_integer_quant.tflite to ONNX\n",
      "2024-02-08 08:34:55,894 - INFO - Model inputs: ['serving_default_images:0']\n",
      "2024-02-08 08:34:55,894 - INFO - Model outputs: ['PartitionedCall:0', 'PartitionedCall:1']\n",
      "2024-02-08 08:34:55,894 - INFO - ONNX model is saved at Models/Fitment_Prediction/best_integer_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --tflite Models/Fitment_Prediction/best_saved_model/best_integer_quant.tflite --output Models/Fitment_Prediction/best_integer_quant.onnx --opset 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def inference(Image):\n",
    "    sess = ort.InferenceSession(\"Models/768Efficientnetv2B0_quint8.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "    return sess.run([\"StatefulPartitionedCall:0\"], {\"serving_default_input_1:0\": Image})[0][0]\n",
    "\n",
    "futures = [inference.remote(i) for i in X_test]\n",
    "# Retrieve results.\n",
    "pred_arr = ray.get(futures)\n",
    "ray.shutdown()\n",
    "y_pred = np.argmax(pred_arr, axis=1)\n",
    "y_pred = [label[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Ray Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sess = ort.InferenceSession(\"Models/Fitment_Prediction/best_int8.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "results_ort = sess.run([\"PartitionedCall:0\", \"PartitionedCall:1\"], {\"serving_default_images:0\": np.expand_dims(X_test[0], axis=0)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = str(dim) + 'Efficientnetv2B0_quint8'\n",
    "model_path = 'Models/'\n",
    "with Live(exp_name=model_name, resume=True) as live:\n",
    "    live.log_artifact('Models/768Efficientnetv2B0_quint8.onnx', type='model', name=model_name)\n",
    "    live.log_sklearn_plot(\"confusion_matrix\", y_test, y_pred, name='quant_onnx', title='quant_onnx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools\n",
    "# import onnx_coreml\n",
    "coreml_model = coremltools.converters.onnx.convert(\"Models/768Efficientnetv2B0_quint8.onnx\",\n",
    "                                                    mode='classifier',\n",
    "                                                    class_labels=['Left', 'Lower', 'Right', 'Smile', 'Upper']\n",
    "                                                    )\n",
    "# coreml_model.spec.description.output[0].type.multiArrayType.dataType = coremltools.proto.FeatureTypes_pb2.ArrayFeatureType.BOOL\n",
    "\n",
    "coreml_model.save(\"Models/768Efficientnetv2B0_quint8.mlmodel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/newvolume/mybyte/Joel/venv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.10 ðŸš€ Python-3.10.9 torch-2.1.2+cu121 CPU (AMD EPYC 7R32)\n",
      "YOLOv8x-seg summary (fused): 295 layers, 71724508 parameters, 0 gradients, 343.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'Models/Fitment Prediction/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 40, 8400), (1, 32, 160, 160)) (137.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.13.1...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.35...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 19.6s, saved as 'Models/Fitment Prediction/best.onnx' (273.9 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m running 'onnx2tf -i \"Models/Fitment Prediction/best.onnx\" -o \"Models/Fitment Prediction/best_saved_model\" -nuo --verbosity info -oiqt -qt per-tensor'\n",
      "\n",
      "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
      "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
      "\n",
      "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
      "\n",
      "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
      "\u001b[33mWARNING:\u001b[0m The optimization process for shape estimation is skipped because it contains OPs that cannot be inferred by the standard onnxruntime.\n",
      "\u001b[33mWARNING:\u001b[0m module 'onnx' has no attribute '_serialize'\n",
      "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
      "\u001b[32msaved_model output complete!\u001b[0m\n",
      "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
      "\u001b[32mFloat16 tflite output complete!\u001b[0m\n",
      "\u001b[34mInput signature information for quantization\u001b[0m\n",
      "\u001b[34msignature_name\u001b[0m: serving_default\n",
      "\u001b[34minput_name.0\u001b[0m: images \u001b[34mshape\u001b[0m: (1, 640, 640, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'>\n",
      "\u001b[32mDynamic Range Quantization tflite output complete!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINT8 Quantization tflite output complete!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mFull INT8 Quantization tflite output complete!\u001b[0m\n",
      "\u001b[32mINT8 Quantization with int16 activations tflite output complete!\u001b[0m\n",
      "\u001b[32mFull INT8 Quantization with int16 activations tflite output complete!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 07:28:22.460160: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-02-08 07:28:22.460260: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 535.129.3 does not match DSO version 535.154.5 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 959.9s, saved as 'Models/Fitment Prediction/best_saved_model' (891.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.13.1...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as 'Models/Fitment Prediction/best_saved_model/best_int8.tflite' (68.8 MB)\n",
      "\n",
      "Export complete (965.6s)\n",
      "Results saved to \u001b[1m/newvolume/mybyte/Joel/MyByte-Face_ID_Screening/Models/Fitment Prediction\u001b[0m\n",
      "Predict:         yolo predict task=segment model=Models/Fitment Prediction/best_saved_model/best_int8.tflite imgsz=640 int8 \n",
      "Validate:        yolo val task=segment model=Models/Fitment Prediction/best_saved_model/best_int8.tflite imgsz=640 data=/newvolume/mybyte/subbiah/MyByte-ML_Model_Training-2/data.yaml int8 \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Models/Fitment Prediction/best_saved_model/best_int8.tflite'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('Models/Fitment Prediction/best.pt')\n",
    "\n",
    "# Export the model to ONNX format\n",
    "model.export(format='tflite', int8=True)  # creates 'yolov8n.onnx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40*8400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*160*160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
